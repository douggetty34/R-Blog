---
title: "Protecting against sketchy research practices: p-values, effect sizes, and power analyses"
author: "Doug Getty"
date: "2019-05-18"
output: html_document
tags: ["power analysis","p-hacking","lme4"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE)
```

In my statistics class this Spring, we had a discussion about scientific reproducibility and practices that can lead to p-hacking. In particular, we talked about the value of running power analyses before a study to estimate a sufficient sample size. 
  
Two points came up in the class discussion that I have been interested to test on a real dataset. The first point is that p-values for a model can fluctuate quite dramatically with the addition of each new piece of data. Without an a priori defined sample size, this can lead researchers to collect a set of data, run an analysis, and then collect more data if the p-value is not significant. In the biz, this practice is widely considered p-hacking. The second point is that, in the absence of prior literature on an effect, researchers can collect data from a small pilot sample and then use the effect size from that sample as an estimate for the actual effect size.  From there, the researcher can do a power analysis to estimate the required sample size to detect a significant effect. In the simplest cases the only thing needed to run a power analysis is an estimate of the effect size of interest. Having a good estimate of the effect size allows researchers to conduct a power analysis and define a sample size before data collection and (ideally) protects against one type of p-hacking. 

These points generated some questions for me: 1) In my own wild caught data, how much does the p-value actually fluctuate as sample size increases? If I analyzed my data with every new subject, is it possible that the p-value could dip below .05 and then become non-signicant again? 2) If the p-value is variable, could the effect size also be variable? And if the effect size is highly variable, using the effect size from a pilot sample might not be a very useful estimate for the true effect size.
  
```{r, message=FALSE}
library(tidyverse)
library(lme4)
library(lmerTest)

data <- read_csv("data_clean.csv") %>%
  mutate(ItemID = as_factor(ItemID),
         Condition = as_factor(Condition))
```

To answer the both questions, I wrote a function in R that runs a regression model on progressively larger and larger datasets. It begins with participants 1 through 8, then adds participants one at a time until the full dataset participants is included. The function saves the p-value and the effect size for each sample size.

```{r}
# Function to save p-values and effect sizes 
model.info.extractor <- function(dataset, filter.low, filter.high, model.formula) {
  
  n <- length(unique(dataset$SubjectID))
  Subs <- seq(from = 8, to = n, by = 1)
  data <- filter(dataset, logRT > log(filter.low), logRT < log(filter.high))
  
  # Initialize output
  output <- data.frame(Subs = vector("integer", length(Subs)), 
                       p.val = vector("double", length(Subs)), 
                       effect = vector("double", length(Subs)))
  
  for (i in 1:length(Subs)) {
    
    temp.data <- data %>%
      filter(SubjectID <= Subs[i])
    
    contrasts(temp.data$Condition) <- c(-1/2,1/2)
    model <- lmer(model.formula, data = temp.data)


    output$Subs[i] <- Subs[i]
    output$p.val[i] <- coef(summary(model))[,"Pr(>|t|)"]["Condition1"]
    output$effect[i] <- coef(summary(model))[,"Estimate"]["Condition1"]
  }
  return(output)
}
```

The first argument of the function is the dataset we want to extract p-values and effect sizes from. My data is made up of reaction times, and many of them are unrealistically short or long. So, I added arguments to the function that allow me to specify how I want to filter the reaction times. I also added an argument to specify the formula of the model I would like to test. 

## Just how variable are p-values?
```{r, message=FALSE}
# Change <- model.info.extractor(data, 150, 2000,
#                                       logRT ~ Condition + logTrial + Duration
#                                       + (1 + Condition + logTrial|SubjectID)
#                                       + (1|ItemID))
# write_csv(Change, "Change.csv")

Change <- read.csv("Change.csv")
Change
```
The function takes quite a while to run, so I ran it once and then saved the results in .csv to save myself 10 minutes every time I wanted to run this document. I left the function call as a comment in the text above.

After running the function with the arguments above, I used ggplot to graph the p-values in order to answer my first question about how the p-value behaves over time. 

```{r}
ggplot(Change, aes(x = sample, y = p.val)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = .05, color = "red") +
  xlab("Cumulative Participants") + ylab("p-value") +
  theme_bw()
```

For the first 30 or so participants, the p-value fluctates a lot with each new participant but generally seems to decrease. After that, it looks like there is less fluctuation and no certain trend in one direction or another. Between participants 50 through 55, the p-value momentarily dips below the .05 threshold and, for a brief moment, I had sweet, sweet scientific glory in my grasp. It's easy to imagine that if I were collecting data without a pre-defined target sample size, I might have just stopped here and sent my paper off to Nature. Unfortunately for my CV, this did not happen :(
  
##Is the effect size from a small pilot sample a good estimate for the true effect size?
Given the variability of p-values as the sample size grows, let's see if the size of the effect is similarly variable over the course of a study. As mentioned earlier, it is common practice to use the effect from a small pilot sample to estimate a sample size using a power analysis. But if effect sizes are really variable, a power analysis might not yield a reliable sample size from a small sample. 

On one hand, if we underestimate the true effect, a power analysis would lead us to collecting a larger sample than necessary. While this is not necessarily bad, it can cause practical problems for projects that are costly or time-intensive. Alternatively, if we overestimate the true effect, a power analysis would lead us to collect a smaller sample than necessary. This would lead researchers not to detect effects when they actually do exist, which is definitely not good. 
  
```{r}
ggplot(Change, aes(x = sample, y = effect)) +
  geom_point() +
  geom_line() +
  geom_hline(yintercept = Change$effect[1], color = "blue") +
  xlab("Cumulative Participants") + ylab("Effect of Condition") +
  theme_bw()
```

The blue line here represents the effect of `r Change$effect[1]` for subjects 1-8, which is the value that I would have used to estimate the true effect size had I done a power analysis. Just from inspecting the graph, it seems that the effect behaves similarly to the p-value in that it is volatile at first and then stabilizes. This makes sense given that the p-value is derived in part from the effect. The biggest difference from p-value variability is that it seems harder to argue that there is a clear directional trend. 

Notably, the effect size of `r Change$effect[1]` for subjects 1-8 is not very far off from the effect size of `r Change$effect[length(Change$effect)]` for the whole sample of 66 subjects. In my case, I would have been lucky to use the effect for subjects 1-8. But the volatility of the effect size early on seems generally problematic for the strategy of using effect sizes from small samples as estimates of the true effect size. 

Presumably the order of my participants was random, so the early effect size volatility should be true no matter the order of the subjects. We can test this by randomly drawing a large number of hypothetical pilot participant groups from the dataset and seeing how often the effect is close to the one we observed above.

```{r}
effect.size.extractor <- function(dataset, sims, model.formula) {
  
  n <- length(unique(dataset$SubjectID))
  
  # Initialize output
  output <- data.frame(SampleNum = vector("integer", sims), 
                       p.val = vector("double", sims), 
                       effect = vector("double", sims))
  
  for (i in 1:sims) {
    
    #Random Subset of Subjects
    data.filt <- dataset %>%
      ungroup() %>%
      select(SubjectID) %>%
      distinct() %>%
      #assign random number to each subject
      mutate(RandSub = sample(1:n, n, replace = F)) %>%
      right_join(data, by = "SubjectID") %>%
      #keep only random subjects 1-8
      filter(RandSub <= 8) %>%
      #apply desired filters
      filter(logRT > log(150), logRT < log(2000))
    
    #Run the Model
    contrasts(data.filt$Condition) <- c(-1/2,1/2)
    model <- lmer(model.formula, data = data.filt)

    #Store the output
    output$SampleNum[i] <- i
    output$p.val[i] <- coef(summary(model))[,"Pr(>|t|)"]["Condition1"]
    output$effect[i] <- coef(summary(model))[,"Estimate"]["Condition1"]
  }
  return(output)
}

```

The above function runs a for loop that takes random subsets of 8 from the sample of 66 subjects, fits a regression model to that subset, and then stores the p.value and effect coefficient from the model. Let's do this 1000 times and then look at the resulting distribution of effect sizes. For my sake, I'll run it on a model without `Duration` as a fixed effect to help the function run a bit more quickly. 

```{r, echo=TRUE, results='hide', warning=FALSE}
# resample <- effect.size.extractor(dataset = data, sims = 1000, model.formula = logRT ~ Condition + logTrial  
#                                   + (1 + Condition + logTrial|SubjectID) + (1|ItemID))
# write_csv(resample, "resample.csv")
resample <- read.csv("resample.csv")
```
Similar to above, I saved the output to a .csv and then reloaded the file to save myself the time it takes for the function to run. 

```{r}
ggplot(resample, aes(x= effect)) +
  geom_histogram(binwidth = .005) +
  geom_vline(xintercept = mean(resample$effect), color = "red") +
  geom_vline(xintercept = Change$effect[59], color = "blue") +
  theme_bw()
```
The effects are centered around `r mean(resample$effect)` (blue line) which is close to the actual effect of `r Change$effect[59]` from our full sample (red line). This is to be expected, as modelling a large number of small subsets effectively approximates one model with the full dataset. I expect with an infinite number of resamplings, these two would converge.  However, the effects do seem to vary considerably around this number. If I had collected my data from subjects in a different order, I could have estimated an effect size ranging from -.10 to nearly 0.15 in a pilot sample.

To visualize this pilot sample variation relative to how the effect sizes varied in the full data, I've added a rug plot of the resampled effect sizes to the y-axis of the line graph below. 
```{r}
ggplot(Change) +
  geom_point(aes(x = sample, y = effect)) +
  geom_line(aes(x = sample, y = effect)) +
  geom_hline(yintercept = Change$effect[1], color = "blue") +
  xlab("Cumulative Participants") + ylab("Effect of Condition") +
  geom_rug(data = resample, aes(y = effect), col=rgb(.5,0,0,alpha=.15)) +
  theme_bw()
```

  
##tl;dr
P-values seem to be quite variable over the course of data collection. This highlights the need for a good a priori sample size or data collection stopping rule that is independent of the p-value. Power analyses are a good way to come up with an a priori sample size. In the absence of a good estimate of effect size in the prior literature, we can use a small pilot sample to generate an estimate. The method of using a small pilot sample to estimate the true effect size might be necessary in some situations. In the absence of prior literature on an effect, a small pilot sample can be a useful estimate of effect size.  
  
##Additional Questions
1. In my dataset, the p-value dipped below .05 twice. If I had collected my participants in a different order, would this have happened?
2. Does this happen on other datasets? Could I create simulated data with the same parameters and examine whether or not the p-value might behave similiarly? 
3. How would increasing the pilot sample size by a few participants influence the accuracy of the effect size estimate?




